"use server";

import { createAI, createStreamableValue } from 'ai/rsc';
import { config } from './config';
import { functionCalling } from './function-calling';
import { getSearchResults, getImages, getVideos } from './tools/searchProviders';
import { get10BlueLinksContents, processAndVectorizeContent } from './tools/contentProcessing';
import { setInSemanticCache, clearSemanticCache, initializeSemanticCache, getFromSemanticCache } from './tools/semanticCache';
import { relevantQuestions } from './tools/generateRelevantQuestions';
import { streamingChatCompletion } from './tools/streamingChatCompletion';
import { checkRateLimit } from './tools/rateLimiting';
import { lookupTool } from './tools/mentionTools';

async function myAction(userMessage: string, mentionTool: string | null, logo: string | null, file: string): Promise<any> {
  "use server";
  console.log('üöÄ myAction called with:', { userMessage, mentionTool, logo, file });
  
  const streamable = createStreamableValue({});

  (async () => {
    try {
      console.log('üîÑ Starting action processing...');
      
      // Rate limit check
      console.log('üîÑ Checking rate limit...');
      await checkRateLimit(streamable);
      console.log('‚úÖ Rate limit check passed');

      // Initialize semantic cache
      console.log('üîÑ Initializing semantic cache...');
      await initializeSemanticCache();
      console.log('‚úÖ Semantic cache initialized');

      // Check for cached data
      console.log('üîÑ Checking for cached data...');
      const cachedData = await getFromSemanticCache(userMessage);
      if (cachedData) {
        console.log('üíæ Found cached data, returning early');
        streamable.update({ cachedData });
        streamable.done({ llmResponseEnd: true });
        return;
      }
      console.log('‚úÖ No cached data found, proceeding with fresh search');

      if (mentionTool) {
        console.log('üîß Processing mention tool:', mentionTool);
        await lookupTool(mentionTool, userMessage, streamable, file);
        streamable.done({ llmResponseEnd: true });
      } else {
        console.log('üîç Processing regular search...');
        
        // Parallel fetch of search data with individual error handling
        console.log('üîÑ Fetching search data in parallel...');
        
        const searchPromises = [
          getImages(userMessage).catch(err => {
            console.error('‚ùå Error fetching images:', err);
            return [];
          }),
          getSearchResults(userMessage).catch(err => {
            console.error('‚ùå Error fetching search results:', err);
            return [];
          }),
          getVideos(userMessage).catch(err => {
            console.error('‚ùå Error fetching videos:', err);
            return [];
          }),
          functionCalling(userMessage).catch(err => {
            console.error('‚ùå Error in function calling:', err);
            return null;
          }),
        ];

        const [images, sources, videos, conditionalFunctionCallUI] = await Promise.all(searchPromises);

        console.log('üìä Search data fetched:', {
          images: images?.length || 0,
          sources: sources?.length || 0,
          videos: videos?.length || 0,
          hasFunctionCall: !!conditionalFunctionCallUI,
          imagesType: typeof images,
          sourcesType: typeof sources,
          videosType: typeof videos
        });

        // Log the actual data structure
        console.log('üîç Detailed search results:', {
          imagesData: images,
          sourcesData: sources,
          videosData: videos
        });

        // Update with search results
        console.log('üîÑ Updating stream with search results...');
        const searchUpdate = { searchResults: sources, images, videos };
        console.log('üì§ Sending search update:', searchUpdate);
        streamable.update(searchUpdate);
        console.log('‚úÖ Search results sent to stream');

        if (config.useFunctionCalling && conditionalFunctionCallUI) {
          console.log('üîÑ Updating stream with function call UI...');
          streamable.update({ conditionalFunctionCallUI });
          console.log('‚úÖ Function call UI sent to stream');
        }

        // Process content
        console.log('üîÑ Processing content...');
        const html = await get10BlueLinksContents(sources);
        console.log('üìÑ HTML content length:', html?.length || 0);
        
        const vectorResults = await processAndVectorizeContent(html, userMessage);
        console.log('üî¢ Vector results:', vectorResults?.length || 0);
        
        console.log('ü§ñ Starting LLM response...');
        const accumulatedLLMResponse = await streamingChatCompletion(userMessage, vectorResults, streamable);
        console.log('‚úÖ LLM response completed, length:', accumulatedLLMResponse?.length || 0);
        
        console.log('‚ùì Generating follow-up questions...');
        const followUp = await relevantQuestions(sources, userMessage);
        console.log('‚ùì Follow-up questions generated:', followUp?.choices?.length || 0);

        console.log('üîÑ Updating stream with follow-up...');
        streamable.update({ followUp });
        console.log('‚úÖ Follow-up sent to stream');

        // Cache the results
        console.log('üíæ Caching results...');
        setInSemanticCache(userMessage, {
          searchResults: sources,
          images,
          videos,
          conditionalFunctionCallUI: config.useFunctionCalling ? conditionalFunctionCallUI : undefined,
          llmResponse: accumulatedLLMResponse,
          followUp,
          semanticCacheKey: userMessage
        });
        console.log('‚úÖ Results cached');

        console.log('‚úÖ Action completed successfully');
        streamable.done({ llmResponseEnd: true });
      }
    } catch (error) {
      console.error('‚ùå Action error:', error);
      console.error('Error details:', {
        name: (error as Error).name,
        message: (error as Error).message,
        stack: (error as Error).stack
      });
      
      // Update with error information
      streamable.update({ 
        llmResponse: 'An error occurred while processing your request. Please try again.',
        status: 'error'
      });
      
      try {
        streamable.done({ llmResponseEnd: true });
      } catch (closeError) {
        console.error('‚ùå Error closing stream:', closeError);
      }
    }
  })();

  return streamable.value;
}

const initialAIState: {
  role: 'user' | 'assistant' | 'system' | 'function';
  content: string;
  id?: string;
  name?: string;
}[] = [];

const initialUIState: {
  id: number;
  display: React.ReactNode;
}[] = [];

export const AI = createAI({
  actions: {
    myAction,
    clearSemanticCache
  },
  initialUIState,
  initialAIState,
});
